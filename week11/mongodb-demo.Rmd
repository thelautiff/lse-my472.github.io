---
title: "MongoDB-demo"
author: "Akitaka Matsuo and Pablo Barbera"
date: "27 November, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE)
```


## Start MongoDB (in Mac)

```{bash eval=FALSE}
# install mongo using Homebrew
# if you don't have Homebrew installed already, then start
# by running the code line below in the Terminal:
# /usr/bin/ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"
brew tap mongodb/brew
brew install mongodb-community@4.2

# start mongo server
brew services start mongodb-community

# to stop server
#brew services stop mongodb-community
```

## Start MongoDB (in Windows)

Follow the instruction for communitiy edition here:
https://docs.mongodb.com/manual/tutorial/install-mongodb-on-windows/


## Creating a Mongo database

This will replicate what we did using SQLite.

The dataset we will work with is all Facebook posts by Members of the U.S. Congress in 2017, as collected from the public Pages API while it was available. This is the same data we used in week 10. You can download the data from Moodle, in case you have not done so already.


```{r}
library(dplyr)
library(mongolite)
library(microbenchmark)
```

```{r, eval=FALSE}
# create a collection
db <- mongo("facebook")
```

Since mongoDB does not assume the complex structure of multiple tables, we will combine all files and create a collection with all information in one table.

```{r, eval = FALSE}
# read the congress data first
congress <- read.csv("~/data/my472/congress-facebook-2017.csv",
	stringsAsFactors=F)
congress$screen_name <- as.character(congress$screen_name)
```

We will open the files one by one, merge them with the congress table, and then __append__ them to the collection. To speed up the processing, we will use the `data.table` package, but this could be done with other packages.

```{r, eval=FALSE}
library(data.table)
library(stringi)

fls <- list.files("~/data/my472/posts", full.names=TRUE)

# convert to Data.table (does so in place)
setDT(congress)

for (f in fls){
  
  message(f)
  # read file into memory
  fb <- fread(f, stringsAsFactors=F, encoding="UTF-8")  # instead of fb <- read.csv(f, stringsAsFactors=F)
  # format values 
  fb$screen_name <- as.character(fb$screen_name)
  fb$datetime <- readr::parse_datetime(fb$datetime)
  
  fb <- congress[fb, on="screen_name"]  # equivalent to fb <- merge(fb, congress, by="screen_name", all.x=T)
  fb$message <- stri_unescape_unicode(fb$message)  # some encoding errors detected, need to unescape to input the data
  db$insert(fb)
  
}

# testing that it works
## dbGetQuery(dbsql, 'SELECT * FROM posts LIMIT 5')
db$find('{}', limit=5) %>% str()  # '{}' indicates everything
db$count()  # count the number of documents

# if you need to undo, remove all documents with:
#db$remove('{}')

db$disconnect()
```

## Querying a Mongo database

Now that we have our documents in the database, let's see how we can query them. First we connect to the database using `mongo` and then query either using `*$find()` (for simple queries) or `*$aggregate()` (for more complex queries). We can also use the `*$count()` method.

```{r}
db <- mongo('facebook')
## test <- dbGetQuery(dbsql, 'SELECT * FROM congress LIMIT 5')
test <- db$find('{}', limit=5) # '{}' indicates everything
str(test)

# For comparison, we will also use the SQLite database
library(DBI)
dbsql <- dbConnect(RSQLite::SQLite(), "~/data/my472/facebook-db.sqlite")

```

Let's start with some examples of __SELECT__:

```{r}
# querying just one column
db$find(query = '{}', fields = '{"name": true}', limit = 10)

dbGetQuery(dbsql, "SELECT name FROM congress LIMIT 10")
```

Notice the field `_id` in the Mongo data. This is the primary key for each document and can be treated as an index.

The SQL `WHERE` is the first argument of `*$find()` and then a list of variables are specified in the `fields` argument with BSON. `limit` is another argument.

```{r}
# selecting based on values of a column
dbGetQuery(dbsql, "SELECT from_name, type, date
           FROM posts
           WHERE date > '2017-01-01'
           LIMIT 10")

## specifying the date is a bit too complicated. Basically, what it does is convert the 
## date into epoch milliseconds
d <- as.integer(as.POSIXct(strptime("2017-01-01", "%Y-%m-%d"))) * 1000
db$find(query = paste0('{"datetime": {"$gt": {"$date": {"$numberLong": "', d, '" } } } }'), 
        fields = '{"from_name": true, "type": true, "date": true}', 
        limit = 10)

# AND operator
dbGetQuery(dbsql, "SELECT from_name, type, date, likes_count 
           FROM posts
           WHERE type != 'photo' AND likes_count > 500
           LIMIT 10")
db$find(query = '{"type": {"$ne": "photo"}, "likes_count": {"$gt": 500}}', 
        fields = '{"from_name": true, "type": true, "date": true, "likes_count": true}', 
        limit = 10)

# OR operator
dbGetQuery(dbsql, "SELECT from_name, type, date, comments_count 
           FROM posts
           WHERE  type = 'photo' OR type = 'video'
           LIMIT 10")
db$find(query = '{"$or": [{"type": "photo"}, {"type": "video"}]}', 
        fields = '{"from_name": true, "type": true, "date": true, "likes_count": true}', 
        limit = 10)

# membership, IN
dbGetQuery(dbsql, "SELECT from_name, type, date, comments_count 
           FROM posts
           WHERE type IN ('video', 'event')
           LIMIT 10")
db$find(query = '{"type": {"$in": ["photo",  "video"]}}', 
        fields = '{"from_name": true, "type": true, "date": true, "likes_count": true}', 
        limit = 10)

# MongoDB does support regular expressions
# We can use regular expressions! For the options, see https://docs.mongodb.com/manual/reference/operator/query/regex/
dbGetQuery(dbsql, "SELECT from_name, type, date, comments_count 
           FROM posts
           WHERE date LIKE '2017-01-__'
           LIMIT 10")
db$find(query = '{"date": {"$regex": "2017-01-.{2}", "$options": "i"}}', 
        fields = '{"from_name": true, "type": true, "date": true, "likes_count": true}', 
        limit = 10)

dbGetQuery(dbsql, "SELECT from_name, type, date, comments_count 
           FROM posts
           WHERE date LIKE '2017-01%'
           LIMIT 10")
db$find(query = '{"date": {"$regex": "2017-01-.+", "$options": "i"}}', 
        fields = '{"from_name": true, "type": true, "date": true, "likes_count": true}', 
        limit = 10)

dbGetQuery(dbsql, "SELECT from_name, message, date
           FROM posts
           WHERE message LIKE '%london%'
           LIMIT 1")
db$find(query = '{"message": {"$regex": "london", "$options": "i"}}', 
        fields = '{"from_name": true, "type": true, "date": true, "likes_count": true, "message": true}', 
        limit = 10)
```


When some aggretation is involved (e.g. `COUNT` or `GROUP BY`), use `*$aggregate()`. The equivalent of the SQL __GROUP BY__ is `$group`. 

```{r}
dbGetQuery(dbsql, 
  "SELECT from_name, COUNT(*) AS post_count
  FROM posts
  GROUP BY from_name
  LIMIT 10")
db$aggregate('[ {"$group": {"_id": "$from_name", "count": {"$sum": 1}}},
                  {"$limit": 10}]')
## conditional aggregate
db$aggregate('[{ "$match": {"party": "Republican"}}, 
                  {"$group": {"_id": "$from_name", "count": {"$sum": 1}}},
                  {"$limit": 10}]')

```

Like __ORDER BY__, we can use `"$sort"` after find or aggregate.

```{r}
# sort by type_count
dbGetQuery(dbsql, 
  "SELECT type, COUNT(type) AS type_count
  FROM posts
  GROUP BY type
  ORDER BY type_count")
db$aggregate('[{"$group": {"_id": "$i_type", "type_count": {"$sum": 1}}},
                  {"$sort": {"type_count": 1}}]')

# now in descending orders
dbGetQuery(dbsql, 
  "SELECT type, COUNT(type) AS type_count
  FROM posts
  GROUP BY type
  ORDER BY type_count DESC")
db$aggregate('[{"$group": {"_id": "$i_type", "type_count": {"$sum": 1}}},
                  {"$sort": {"type_count": -1}}]')

# which was the most popular post?
dbGetQuery(dbsql, 
  "SELECT from_name, message, likes_count, datetime
  FROM posts
  ORDER BY likes_count DESC
  LIMIT 1")
db$find(query = '{}',
             field = '{"from_name": true, "message": true, "likes_count": true, "datetime": true}',
             sort = '{"likes_count": -1}',
             limit = 1)
                  #{"$sort": {"type_count": -1}}]')

# what was the post with the highest comment to like ratio? We subset only posts with 1000 likes or more to avoid outliers.
dbGetQuery(dbsql,
  "SELECT from_name, message, likes_count, comments_count, date,   
      comments_count/likes_count AS comment_like_ratio
  FROM posts
  WHERE likes_count > 1000
  ORDER BY comment_like_ratio DESC
  LIMIT 5")

db$aggregate('[{ "$match" : {"likes_count": {"$gt": 1000}}},
                  {"$project": {"from_name": 1, "message": 1, "likes_count": 1, "comments_count": 1, "date": 1,
                  "comment_like_ratio": {"$divide": ["$comments_count", {"$add": ["$likes_count", 1]}]}}},
                  {"$sort": {"comment_like_ratio": -1}},
                  {"$limit": 5}]') 
```



## Performance?

For both databases, we haven't done any tunings (e.g. indexing). But let's compare which is faster just for fun.

```{r}

microbenchmark(sqlite = 
  dbGetQuery(dbsql, "SELECT from_name, type, date, likes_count 
           FROM posts
           WHERE type != 'photo' 
              AND likes_count > 500
           LIMIT 10"),
  mongo = db$find(query = '{"type": {"$ne": "photo"}, "likes_count": {"$gt": 500}}', 
        fields = '{"from_name": true, "type": true, "date": true, "likes_count": true}', 
        limit = 10), times = 10)
```

```{r}
microbenchmark(sqlite = 
  dbGetQuery(dbsql,
    "SELECT from_name, message, likes_count, comments_count, date,   
        comments_count/likes_count AS comment_like_ratio
    FROM posts
    WHERE likes_count > 1000
    ORDER BY comment_like_ratio DESC
    LIMIT 5"),
  mongo = db$aggregate('[{ "$match" : {"likes_count": {"$gt": 1000}}},
                  {"$project": {"from_name": 1, "message": 1, "likes_count": 1, "comments_count": 1, "date": 1,
                  "comment_like_ratio": {"$divide": ["$comments_count", {"$add": ["$likes_count", 1]}]}}},
                  {"$sort": {"comment_like_ratio": -1}},
                  {"$limit": 5}]'),
times = 10)
```
We need more tuning for mongo (e.g. add index, etc.), but not bad...


