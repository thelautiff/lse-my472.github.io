---
title: "MongoDB-demo"
author: "Akitaka Matsuo and Pablo Barbera"
date: "27 November, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE)
```


## Start MongoDB (in Mac)

```{bash eval=FALSE}
## install mongo
brew install mongodb

## start mongo server
brew services start mongodb

## to stop server
# brew services stop mongodb
```

## Start MongoDB (in Windows)

Follow the instruction for communitiy edition here:
https://docs.mongodb.com/manual/tutorial/install-mongodb-on-windows/


## Creating a Mongo database

This will replicate what we did using SQLite.

The dataset we will work with is all Facebook posts by Members of the U.S. Congress in 2017, as collected from the public Pages API while it was available. You can download the data from Moodle.


```{r}
library(mongolite)
library(microbenchmark)
```

```{r, eval=FALSE}
# create collection
db <- mongo("facebook")
```

Since mongoDB does not assume the complex structure of multiple tables, we will combine all files and create a collection with all information in one table.

```{r, eval = FALSE}
# read the congress data first
congress <- read.csv("~/data/congress-facebook-2017.csv",
	stringsAsFactors=F)
```

We will open the files one by one, merge them with the congress table, and then __append__ them to the collection. To speed up the processing, I will use the `data.table` package, but this could be done with other packages.

```{r, eval=FALSE}
library(data.table)
library(stringi)

fls <- list.files("~/data/posts", full.names=TRUE)

# Let's check the speed difference

congress$screen_name <- as.character(congress$screen_name)
# convert to Data.table (does so in place)
setDT(congress)

for (f in fls){
  
  message(f)
  # read file into memory
  fb <- fread(f, stringsAsFactors=F, encoding="UTF-8")
  fb$screen_name <- as.character(fb$screen_name)
  fb$datetime <- readr::parse_datetime(fb$datetime)
  #fb <- read.csv(f, stringsAsFactors=F)
  
  fb <- congress[fb, on="screen_name"] ## this is equivalent to merge(fb, congress, by="screen_name", all.x=T)
  fb$message <- stri_unescape_unicode(fb$message) ## some encoding error detected, need to unescape to input in the data
  db$insert(fb)
  
}
# testing that it works
## dbGetQuery(dbsql, 'SELECT * FROM posts LIMIT 5')
db$find('{}', limit=5) %>% str()  # '{}' indicates everything
db$disconnect()
```

## Querying a Mongo database

Now that we have our documents in the database, let's see how we can query them. First we connect to the database using `mongo` and then query either using `*$find()` (for simple queries) or `*$aggregate()` (for more complex queries). We can also use the `*$count()` method.

```{r}
db <- mongo('facebook')
#test <- dbGetQuery(dbsql, 'SELECT * FROM congress LIMIT 5')
test <- db$find('{}', limit=5) # '{}' indicates everything
str(test)

# For comparison, we will also use RSQlite database
library(DBI)
dbsql <- dbConnect(RSQLite::SQLite(), "~/data/facebook-db.sqlite")

```

Let's start with some examples of __SELECT__:

```{r}
# querying just one column
db$find('{}', fields = '{"name": true}', limit = 10)

dbGetQuery(dbsql, "SELECT name FROM congress LIMIT 10")
```

SQL `WHERE` is the first argument of `*$find()` and then a list of variables are specified in the `fields` argument with BSON. `limit` is another argument.

```{r}
# selecting based on values of a column
dbGetQuery(dbsql, "SELECT from_name, type, date
           FROM posts
           WHERE date > '2017-01-01'
           LIMIT 10")

## specifying the date is a bit too complicated. Basically what it does is convert the 
## date into epoch milliseconds
d <- as.integer(as.POSIXct(strptime("2017-01-01","%Y-%m-%d"))) * 1000
db$find(query = paste0('{"datetime": {"$gt": {"$date: {"$numberLong": "', d, '" } } } }'), 
        fields = '{"from_name": true, "type": true, "date": true}', 
        limit = 10)

# AND operator
dbGetQuery(dbsql, "SELECT from_name, type, date, likes_count 
           FROM posts
           WHERE type != 'photo' AND likes_count > 500
           LIMIT 10")
db$find(query = '{"type": {"$ne": "photo"}, "likes_count": {"$gt": 500}}', 
        fields = '{"from_name": true, "type": true, "date": true, "likes_count": true}', 
        limit = 10)

# OR operator
dbGetQuery(dbsql, "SELECT from_name, type, date, comments_count 
           FROM posts
           WHERE  type = 'photo' OR type = 'video'
           LIMIT 10")
db$find(query = '{"$or": [{"type": "photo"}, {"type": "video"}]}', 
        fields = '{"from_name": true, "type": true, "date": true, "likes_count": true}', 
        limit = 10)

# membership, IN
dbGetQuery(dbsql, "SELECT from_name, type, date, comments_count 
           FROM posts
           WHERE type IN ('video', 'event')
           LIMIT 10")
db$find(query = '{"type": {"$in": ["photo",  "video"]}}', 
        fields = '{"from_name": true, "type": true, "date": true, "likes_count": true}', 
        limit = 10)

# MongoDB does support regular expressions
# We can use regular expressions!
dbGetQuery(dbsql, "SELECT from_name, type, date, comments_count 
           FROM posts
           WHERE date LIKE '2017-01-__'
           LIMIT 10")
db$find(query = '{"date": {"$regex": "2017-01-.{2}", "$options": "i"}}', 
        fields = '{"from_name": true, "type": true, "date": true, "likes_count": true}', 
        limit = 10)

dbGetQuery(dbsql, "SELECT from_name, type, date, comments_count 
           FROM posts
           WHERE date LIKE '2017-01%'
           LIMIT 10")
db$find(query = '{"date": {"$regex": "2017-01-.+", "$options": "i"}}', 
        fields = '{"from_name": true, "type": true, "date": true, "likes_count": true}', 
        limit = 10)

dbGetQuery(dbsql, "SELECT from_name, message, date
           FROM posts
           WHERE message LIKE '%london%'
           LIMIT 1")
db$find(query = '{"message": {"$regex": "london", "$options": "i"}}', 
        fields = '{"from_name": true, "type": true, "date": true, "likes_count": true, "message": true}', 
        limit = 10)
```


When some aggretation is involved (e.g. `COUNT` or `GROUP BY`),  `*$aggregate()` is used. There is an equivalent of __GROUP BY__ in SQL which is `$group`. 

```{r}
dbGetQuery(dbsql, 
  "SELECT from_name, COUNT(*) AS post_count
  FROM posts
  GROUP BY from_name
  LIMIT 10")
db$aggregate('[ {"$group": {"_id": "$from_name", "count": {"$sum": 1}}},
                  {"$limit": 10}]')
## conditional aggregate
db$aggregate('[{ "$match" : {"party": "Republican"}}, 
                  {"$group": {"_id": "$from_name", "count": {"$sum": 1}}},
                  {"$limit": 10}]')

```

Like __ORDER BY__, we can use `"$sort"` condition, after find or aggregate.

```{r}
# sort by type_count
dbGetQuery(dbsql, 
  "SELECT type, COUNT(type) AS type_count
  FROM posts
  GROUP BY type
  ORDER BY type_count")
db$aggregate('[{"$group": {"_id": "$i_type", "type_count": {"$sum": 1}}},
                  {"$sort": {"type_count": 1}}]')

# now in descending orders
dbGetQuery(dbsql, 
  "SELECT type, COUNT(type) AS type_count
  FROM posts
  GROUP BY type
  ORDER BY type_count DESC")
db$aggregate('[{"$group": {"_id": "$i_type", "type_count": {"$sum": 1}}},
                  {"$sort": {"type_count": -1}}]')

# which was the most popular post?
dbGetQuery(dbsql, 
  "SELECT from_name, message, likes_count, datetime
  FROM posts
  ORDER BY likes_count DESC
  LIMIT 1")
db$find(query = '{}',
             field = '{"from_name": true, "message": true, "likes_count": true, "datetime": true}',
             sort = '{"likes_count": -1}',
             limit = 1)
                  #{"$sort": {"type_count": -1}}]')

# what was the post with the highest comment to like ratio? We subset only posts with 1000 likes or more to avoid outliers.
dbGetQuery(dbsql,
  "SELECT from_name, message, likes_count, comments_count, date,   
      comments_count/likes_count AS comment_like_ratio
  FROM posts
  WHERE likes_count > 1000
  ORDER BY comment_like_ratio DESC
  LIMIT 5")

db$aggregate('[{ "$match" : {"likes_count": {"$gt": 1000}}},
                  {"$project": {"from_name": 1, "message": 1, "likes_count": 1, "comments_count": 1, "date": 1,
                  "comment_like_ratio": {"$divide": ["$comments_count", {"$add": ["$likes_count", 1]}]}}},
                  {"$sort": {"comment_like_ratio": -1}},
                  {"$limit": 5}]') 
# this return error as for some
```



## Performance?

For both databases, we haven't done any tunings. But let's compare which is faster just for fun.

```{r}

microbenchmark(sqlite = 
  dbGetQuery(dbsql, "SELECT from_name, type, date, likes_count 
           FROM posts
           WHERE type != 'photo' 
              AND likes_count > 500
           LIMIT 10"),
  mongo = db$find(query = '{"type": {"$ne": "photo"}, "likes_count": {"$gt": 500}}', 
        fields = '{"from_name": true, "type": true, "date": true, "likes_count": true}', 
        limit = 10), times = 10)
```

```{r}
microbenchmark(sqlite = 
  dbGetQuery(dbsql,
    "SELECT from_name, message, likes_count, comments_count, date,   
        comments_count/likes_count AS comment_like_ratio
    FROM posts
    WHERE likes_count > 1000
    ORDER BY comment_like_ratio DESC
    LIMIT 5"),
  mongo = db$aggregate('[{ "$match" : {"likes_count": {"$gt": 1000}}},
                  {"$project": {"from_name": 1, "message": 1, "likes_count": 1, "comments_count": 1, "date": 1,
                  "comment_like_ratio": {"$divide": ["$comments_count", {"$add": ["$likes_count", 1]}]}}},
                  {"$sort": {"comment_like_ratio": -1}},
                  {"$limit": 5}]'),
times = 10)
```
We need more tuning for mongo (e.g. Add index, etc.), but not bad...


